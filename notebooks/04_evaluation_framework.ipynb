{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Framework\n",
    "\n",
    "This notebook implements the LLM-as-a-Judge evaluation framework to assess domain name suggestions based on relevance, memorability, appropriateness, and availability-style plausibility.\n",
    "\n",
    "## Objectives\n",
    "- Evaluate domain suggestions using LLM-as-a-Judge.\n",
    "- Score suggestions on four metrics.\n",
    "- Aggregate results and generate summary statistics.\n",
    "- Compare performance across model versions.\n",
    "\n",
    "## Setup\n",
    "Ensure the environment is set up and API keys for OpenAI/Anthropic are configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from evaluation.llm_judge import LLMJudge\n",
    "from utils.config import load_config\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration\n",
    "\n",
    "Load the evaluation configuration from `config/evaluation_config.yaml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = 'config/evaluation_config.yaml'\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "EVAL_DATASET_PATH = config['datasets']['test_set']['path']\n",
    "OUTPUT_PATH = 'data/evaluation/results.json'\n",
    "\n",
    "logger.info(f'Loading evaluation configuration from {CONFIG_PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM Judge\n",
    "\n",
    "Initialize the LLM-as-a-Judge evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = LLMJudge(CONFIG_PATH)\n",
    "logger.info('LLM-as-a-Judge initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Dataset\n",
    "\n",
    "Load the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EVAL_DATASET_PATH, 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "logger.info(f'Loaded evaluation dataset with {len(dataset)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Dataset\n",
    "\n",
    "Run the comprehensive evaluation on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_evaluation():\n",
    "    results = await judge.evaluate_dataset(EVAL_DATASET_PATH, OUTPUT_PATH, max_samples=100)\n",
    "    logger.info(f'Evaluation completed. Results saved to {OUTPUT_PATH}')\n",
    "    return results\n",
    "\n",
    "results = asyncio.run(run_evaluation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Analyze the evaluation results and visualize performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "with open(OUTPUT_PATH, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "# Extract scores\n",
    "scores = []\n",
    "for sample in results:\n",
    "    for result in sample['results']:\n",
    "        for metric, eval_result in result['evaluations'].items():\n",
    "            scores.append({\n",
    "                'Sample': sample['sample_id'],\n",
    "                'Metric': metric,\n",
    "                'Score': eval_result['score']\n",
    "            })\n",
    "\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Plot score distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=scores_df, x='Metric', y='Score')\n",
    "plt.title('Score Distribution by Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate summary statistics\n",
    "summary = scores_df.groupby('Metric')['Score'].agg(['mean', 'std', 'min', 'max'])\n",
    "print('Summary Statistics:')\n",
    "print(summary)\n",
    "\n",
    "# Plot overall score distribution\n",
    "overall_scores = [sample['results'][0]['overall_score'] for sample in results]\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(overall_scores, bins=20)\n",
    "plt.title('Overall Score Distribution')\n",
    "plt.xlabel('Overall Score')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The evaluation framework successfully scored domain suggestions across four metrics. The results provide insights into model performance and areas for improvement. The next step is to analyze edge cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}