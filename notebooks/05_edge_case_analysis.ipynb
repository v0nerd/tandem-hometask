{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Case Analysis\n",
    "\n",
    "This notebook analyzes edge cases to identify model weaknesses and improve performance on challenging inputs.\n",
    "\n",
    "## Objectives\n",
    "- Identify common edge cases (ambiguous, non-English, brand overlaps, inappropriate content).\n",
    "- Evaluate model performance on edge cases.\n",
    "- Develop a taxonomy of failures and improvement strategies.\n",
    "\n",
    "## Setup\n",
    "Ensure the environment is set up and the edge case dataset is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "from evaluation.llm_judge import LLMJudge\n",
    "from evaluation.safety_checker import SafetyChecker\n",
    "from utils.config import load_config\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Edge Case Dataset\n",
    "\n",
    "Load the edge case dataset created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_CASE_PATH = 'data/edge_cases/edge_cases.json'\n",
    "CONFIG_PATH = 'config/evaluation_config.yaml'\n",
    "\n",
    "with open(EDGE_CASE_PATH, 'r') as f:\n",
    "    edge_cases = json.load(f)\n",
    "\n",
    "logger.info(f'Loaded edge case dataset with {len(edge_cases)} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Evaluator and Safety Checker\n",
    "\n",
    "Initialize the LLM-as-a-Judge and safety checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = LLMJudge(CONFIG_PATH)\n",
    "safety_checker = SafetyChecker()\n",
    "\n",
    "logger.info('Initialized LLM-as-a-Judge and Safety Checker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Edge Cases\n",
    "\n",
    "Evaluate the edge cases and analyze performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_edge_cases():\n",
    "    results = []\n",
    "    for case in edge_cases:\n",
    "        description = case['input']\n",
    "        category = case['metadata']['category']\n",
    "        domain = case['output']\n",
    "        \n",
    "        # Check safety\n",
    "        safety_result = safety_checker.check_safety(description)\n",
    "        \n",
    "        # Evaluate if safe\n",
    "        eval_results = {'safety': safety_result.__dict__}\n",
    "        if safety_result.is_safe:\n",
    "            eval_results['judge'] = await judge.evaluate_comprehensive(description, [domain])\n",
    "        \n",
    "        results.append({\n",
    "            'description': description,\n",
    "            'category': category,\n",
    "            'domain': domain,\n",
    "            'results': eval_results\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "edge_case_results = asyncio.run(evaluate_edge_cases())\n",
    "\n",
    "# Save results\n",
    "with open('data/evaluation/edge_case_results.json', 'w') as f:\n",
    "    json.dump(edge_case_results, f, indent=2)\n",
    "\n",
    "logger.info('Edge case evaluation completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Edge Case Performance\n",
    "\n",
    "Analyze the results to identify patterns and failure modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame\n",
    "rows = []\n",
    "for result in edge_case_results:\n",
    "    row = {\n",
    "        'Category': result['category'],\n",
    "        'Description': result['description'],\n",
    "        'Domain': result['domain'],\n",
    "        'Is_Safe': result['results']['safety']['is_safe'],\n",
    "        'Risk_Level': result['results']['safety']['risk_level']\n",
    "    }\n",
    "    if result['results']['safety']['is_safe'] and result['results'].get('judge'):\n",
    "        row.update(result['results']['judge'][0]['metric_scores'])\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Plot performance by category\n",
    "metrics = ['relevance', 'memorability', 'appropriateness', 'availability_style']\n",
    "plt.figure(figsize=(12, 6))\n",
    "for metric in metrics:\n",
    "    if metric in df.columns:\n",
    "        sns.boxplot(data=df, x='Category', y=metric)\n",
    "        plt.title(f'{metric.capitalize()} by Edge Case Category')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Summary statistics by category\n",
    "summary = df.groupby('Category').agg({\n",
    "    'relevance': ['mean', 'std'],\n",
    "    'memorability': ['mean', 'std'],\n",
    "    'appropriateness': ['mean', 'std'],\n",
    "    'availability_style': ['mean', 'std'],\n",
    "    'Is_Safe': 'mean'\n",
    "})\n",
    "print('Edge Case Performance Summary:')\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Taxonomy\n",
    "\n",
    "Based on the analysis, the following failure modes were identified:\n",
    "\n",
    "1. **Ambiguous Descriptions**: Low relevance scores due to lack of specific context.\n",
    "2. **Non-English Inputs**: Poor performance due to English-centric training data.\n",
    "3. **Brand Overlaps**: Risk of trademark infringement in suggestions.\n",
    "4. **Inappropriate Content**: Successfully blocked by safety filters.\n",
    "5. **Very Long/Short Descriptions**: Inconsistent performance due to input length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement Strategies\n",
    "\n",
    "1. **Ambiguous Descriptions**: Implement context expansion using keyword extraction.\n",
    "2. **Non-English Inputs**: Add multilingual support and translation preprocessing.\n",
    "3. **Brand Overlaps**: Enhance brand name filtering with a trademark database.\n",
    "4. **Input Length Handling**: Implement intelligent truncation and summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The edge case analysis identified key weaknesses and proposed actionable improvements. These insights will guide the final evaluation and model refinement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}