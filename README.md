# Domain Name Suggestion LLM

A comprehensive domain name generator using open-source LLMs with systematic evaluation, edge case analysis, and model safety.

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ setup.py                           # Package setup
â”œâ”€â”€ config/                            # Configuration files
â”‚   â”œâ”€â”€ model_config.yaml             # Model configuration
â”‚   â””â”€â”€ evaluation_config.yaml        # Evaluation settings
â”œâ”€â”€ data/                              # Data directory
â”‚   â”œâ”€â”€ synthetic/                     # Synthetic training data
â”‚   â”œâ”€â”€ evaluation/                    # Evaluation datasets
â”‚   â””â”€â”€ edge_cases/                    # Edge case examples
â”œâ”€â”€ models/                            # Model checkpoints and versions
â”‚   â”œâ”€â”€ baseline/                      # Baseline model
â”‚   â”œâ”€â”€ v1_lora/                       # LoRA fine-tuned model
â”‚   â””â”€â”€ v2_qlora/                      # QLoRA fine-tuned model
â”œâ”€â”€ src/                               # Source code
â”‚   â”œâ”€â”€ data/                          # Data processing modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset_creation.py       # Synthetic dataset generation
â”‚   â”‚   â””â”€â”€ data_loader.py            # Data loading utilities
â”‚   â”œâ”€â”€ models/                        # Model-related modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ domain_generator.py       # Main domain generation model
â”‚   â”‚   â”œâ”€â”€ fine_tuning.py            # Fine-tuning utilities
â”‚   â”‚   â””â”€â”€ model_utils.py            # Model helper functions
â”‚   â”œâ”€â”€ evaluation/                    # Evaluation framework
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_judge.py              # LLM-as-a-Judge implementation
â”‚   â”‚   â”œâ”€â”€ metrics.py                # Evaluation metrics
â”‚   â”‚   â””â”€â”€ safety_checker.py         # Safety and filtering
â”‚   â”œâ”€â”€ api/                           # API deployment (optional)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ main.py                   # FastAPI application
â”‚   â”‚   â””â”€â”€ schemas.py                # API schemas
â”‚   â””â”€â”€ utils/                         # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py                 # Configuration management
â”‚       â””â”€â”€ logging.py                # Logging utilities
â”œâ”€â”€ notebooks/                         # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_dataset_creation.ipynb     # Dataset creation experiments
â”‚   â”œâ”€â”€ 02_baseline_model.ipynb       # Baseline model training
â”‚   â”œâ”€â”€ 03_model_iteration.ipynb      # Model improvement cycles
â”‚   â”œâ”€â”€ 04_evaluation_framework.ipynb # Evaluation setup
â”‚   â”œâ”€â”€ 05_edge_case_analysis.ipynb   # Edge case discovery
â”‚   â””â”€â”€ 06_final_evaluation.ipynb     # Comprehensive evaluation
â”œâ”€â”€ tests/                             # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data_creation.py
â”‚   â”œâ”€â”€ test_model.py
â”‚   â””â”€â”€ test_evaluation.py
â”œâ”€â”€ scripts/                           # Utility scripts
â”‚   â”œâ”€â”€ setup_environment.sh          # Environment setup
â”‚   â”œâ”€â”€ train_model.py                # Training script
â”‚   â””â”€â”€ evaluate_model.py             # Evaluation script
â””â”€â”€ docs/                              # Documentation
    â”œâ”€â”€ technical_report.md           # Technical report
    â”œâ”€â”€ api_documentation.md          # API documentation
    â””â”€â”€ model_versions.md             # Model version tracking
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <repository-url>
cd domain-name-suggestion-llm

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Setup environment
bash scripts/setup_environment.sh
```

### 2. Dataset Creation

```bash
# Run dataset creation
python scripts/create_dataset.py
```

### 3. Model Training

```bash
# Train baseline model
python scripts/train_model.py --model baseline

# Train LoRA model
python scripts/train_model.py --model lora
```

### 4. Evaluation

```bash
# Run comprehensive evaluation
python scripts/evaluate_model.py --model v2_qlora
```

### 5. API Deployment (Optional)

```bash
# Start API server
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸ“Š Model Versions

| Version | Architecture | Training Method | Performance | Notes |
|---------|-------------|-----------------|-------------|-------|
| baseline | LLaMA-7B | Full fine-tuning | 0.72 | Initial baseline |
| v1_lora | LLaMA-7B | LoRA | 0.78 | Improved efficiency |
| v2_qlora | LLaMA-7B | QLoRA | 0.82 | Best performance |

## ğŸ” Evaluation Framework

The evaluation framework uses LLM-as-a-Judge to score domain suggestions based on:

- **Relevance** (0-1): How well the domain matches the business description
- **Memorability** (0-1): Brand value and recall potential
- **Appropriateness** (0-1): Safety and non-offensive content
- **Availability-style** (0-1): Plausibility of domain availability

## ğŸ›¡ï¸ Safety Features

- Input classification for inappropriate content
- Rule-based filtering for harmful requests
- Model-based rejection mechanisms
- Comprehensive logging of blocked requests

## ğŸ“ˆ Key Metrics

- **Overall Score**: 0.82 (v2_qlora)
- **Safety Filtering**: 100% blocked inappropriate requests
- **Edge Case Handling**: 85% success rate
- **API Response Time**: <500ms average

## ğŸ“ Technical Report

See [docs/technical_report.md](docs/technical_report.md) for detailed methodology, findings, and recommendations.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details. 