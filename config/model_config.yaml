# Model Configuration for Domain Name Suggestion LLM

# Base model settings
base_model:
  name: "meta-llama/Llama-2-7b-hf"
  revision: "main"
  trust_remote_code: false
  use_auth_token: true

# Model versions configuration
versions:
  baseline:
    training_method: "full_fine_tuning"
    learning_rate: 2e-5
    batch_size: 4
    gradient_accumulation_steps: 8
    num_epochs: 3
    warmup_steps: 100
    weight_decay: 0.01
    max_grad_norm: 1.0
    save_steps: 500
    eval_steps: 500
    logging_steps: 100
    
  v1_lora:
    training_method: "lora"
    learning_rate: 3e-4
    batch_size: 8
    gradient_accumulation_steps: 4
    num_epochs: 5
    warmup_steps: 200
    weight_decay: 0.01
    max_grad_norm: 1.0
    save_steps: 500
    eval_steps: 500
    logging_steps: 100
    lora_config:
      r: 16
      lora_alpha: 32
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
      lora_dropout: 0.1
      bias: "none"
      task_type: "CAUSAL_LM"
      
  v2_qlora:
    training_method: "qlora"
    learning_rate: 2e-4
    batch_size: 16
    gradient_accumulation_steps: 2
    num_epochs: 7
    warmup_steps: 300
    weight_decay: 0.01
    max_grad_norm: 1.0
    save_steps: 500
    eval_steps: 500
    logging_steps: 100
    qlora_config:
      r: 32
      lora_alpha: 64
      target_modules: ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      lora_dropout: 0.1
      bias: "none"
      task_type: "CAUSAL_LM"
      bits: 4
      group_size: 128
      double_quant: true

# Generation settings
generation:
  max_length: 128
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  num_return_sequences: 3
  pad_token_id: 0
  eos_token_id: 2

# Data processing
data:
  max_input_length: 512
  max_output_length: 64
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  num_samples: 10000  # For synthetic dataset

# Evaluation settings
evaluation:
  llm_judge:
    model: "gpt-4"  # or "claude-3-sonnet"
    temperature: 0.0
    max_tokens: 1000
  metrics:
    - "relevance"
    - "memorability"
    - "appropriateness"
    - "availability_style"
  aggregation_method: "weighted_average"
  weights:
    relevance: 0.3
    memorability: 0.25
    appropriateness: 0.25
    availability_style: 0.2

# Safety settings
safety:
  blocked_keywords:
    - "adult"
    - "porn"
    - "explicit"
    - "nude"
    - "violence"
    - "hate"
    - "discrimination"
  blocked_domains:
    - ".xxx"
    - ".adult"
  min_confidence_threshold: 0.7
  max_suggestions_per_request: 5

# Logging and monitoring
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/training.log"
  wandb_project: "domain-name-suggestion-llm"
  wandb_entity: null  # Set your wandb username

# Hardware settings
hardware:
  device: "auto"  # "auto", "cuda", "cpu"
  mixed_precision: "bf16"  # "fp16", "bf16", "no"
  gradient_checkpointing: true
  dataloader_num_workers: 4 